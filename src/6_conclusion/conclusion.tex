Knowledge graphs are used in more and more domains and research on AI assisted knowledge graph completion enjoys increasing interest. Due to the ongoing exponential information growth~\cite{}, especially of freely accessible texts on the web~\cite{}, it is becoming easier to obtain additional entity information that can be used to improve the predictions of KGC models.

In this work, the Power ensemble model was developed which uses complementary rule-based and text-based components to determine new facts for a knowledge graph for whose entities textual descriptions are available. The rule-based component exploits patterns in the graph structure found by the rule miner AnyBURL while the text-based component uses the state-of-the-art transformer DistilBERT to effectively evaluate texts. Thanks to the rule-based component and an attention mechanism within the text-based component, the Power model also has the advantage over existing embedding-based models that it can provide human-understandable information about which rules and which texts were decisive for the predictions.

In a series of experiments, the model was evaluated on several combinations of graphs and text sets. Furthermore, different variations of the components were compared and the optimal conditions for training were determined. For the text-based component, this yielded the surprising finding that very good results can already be obtained without the use of computationally intensive transformers. Moreover, the attention mechanism was qualitatively tested for its functionality - and although the attention mechanism does not improve the performance, it nevertheless causes the comprehensible prioritization of the sentences. Overall, it was found that, depending on the graph and text inventory, sometimes the rule-based component and sometimes the text-based component provided better predictions, but the Power model as a whole always provided better results than either component on its own.

Building on the results of this thesis, future studies could address the following aspects in order to further improve the quality of the predicted facts:

\begin{itemize}
    \item The predicted facts could be used as a basis for the inference of further facts by repeatedly applying the known rules to the new facts. The predicted facts would be extended step by step until at some point they would correspond to the hull of all facts derivable from the given facts. Thereby, the probabilities of the underlying premises would have to be taken into account when calculating the probabilities of derived facts. With such an extension, transitive relations, such as ``is part of'' could be fully exploited and the rule-based component could participate in processing open-world entity which lack initially known facts.

    \item The model could be extended to include head prediction, i. e. given an entity $x$, predict facts of the form $(head, rel, x)$ without having to wait for the processing of the entity $head$.

    \item The rule-based component is currently limited to processing the particularly common, reliable AnyBURL rules of type $AC_1$ of length 1, and could be extended to rules of types $AC_2$ and $C$, as well as rules of arbitrary length.

    \item In terms of the ranking scenario, evaluation of rules with a confidence lower than 0.5 could be considered to achieve higher average precision. At the same time, new metrics like precision@k could be considered to check if the increased mAP is also reflected in the top predictions.

    \item It could be investigated whether splitting the classifier used in the text-based component into several smaller classifiers gives an advantage in regard to unbalanced classes.
\end{itemize}
