NLP = natural language processing
machine learning, primer about training, datasets at <link> []

embedding based: embed tokens in (relatively) low-dim vector space,
that has properties like man/woman -> king/queen
word2vec, glove

pre-training
many works showed that pre-training on large general-purpose corpus with
subsequent finetuning yields better results~\cite{}

transformers
attention based
transformer = encoder + decoder
depending on problem one or both better suited, generally good: bert = encoder
bert revolutionalized nlp
