During supervised training, the Texter is fine-tuned to a specific dataset by repeatedly predicting facts, calculating its loss with regard to the ground truth class labels, and performing backpropagation to learn its randomly initialized parameters and fine-tune its word embeddings. The randomly initialized parameters include the classification block's weight matrix and bias vector, and, in case of the attentive Texter, also the attention block's class embeddings.

For the multi-label problem at hand, the binary cross-entropy (BCE) loss function is used. A convenient property of the BCE function is that it produces very large loss values for very wrong predictions which contributes to an initially fast learning process. To counteract unbalanced classes in the Texter dataset, the weighted binary cross entropy loss function (wBCE) shown in Equation~\ref{eq:4_approach/1_texter/4_training/wbce} is used. It calculates the loss for the multi-label output logits $x$ and the respective ground truth labels $y$, both of which are c-dimensional vectors with $c$ being the number of output classes. Without the weights, the model would learn that it gets off best by always making negative predictions for very rare classes. This would be good for high accuracy, but is bad for the metrics used during evaluation, which do not measure true negatives. A class' weight $w_c$ is calculated as the reciprocal of the class' frequency it occurs in the training data with. A class that is true for every fifth entity, for example, would be assigned a class weight of five. To make the logits comparable with the usual 0 and 1 ground truth labels, they are normalized to range $[0, 1]$ by applying the sigmoid function.

\begin{align}
    wBCE(x, y) = - \frac{1}{C} \sum_{c = 1}^C w_c \cdot log(\sigma(x)) + (1 - y) \cdot log(1 - \sigma(x))
    \label{eq:4_approach/1_texter/4_training/wbce}
\end{align}

Besides the loss function, another important aspect of training is the way the gradients calculated from the loss are applied during backpropagation, i.e. the way to perform gradient descent. Plain batch gradient descent is generally too slow and while stochastic gradient descent and mini-batch gradient descent improve in this matter, they still cannot be considered fast. While many projects use the adaptive Adam optimizer, popular for its speed, this work uses the AdamW optimizer, recommended for the finetuning of transformers~\cite{Loshchilov2019DecoupledWD}. According to its inventors, it keeps the speed of Adam, while approaching the generalization capabilities of SGD with momentum.
