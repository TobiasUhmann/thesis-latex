The Texter's task is to take entity texts, which are usually sentences, and infer facts from them. Basically, it does so by taking a query entity's sentences and applying a multi-label classifier whose output classes correspond to facts about the entity. To be precise, the classes correspond to relation-entity tuples that form facts when combined with the query entity. \autoref{fig:4_approach/1_texter/texter_idea} sketches the idea by the example of the entity John from the introductory chapter. In John's case, the Texter would predict the facts $(John, has~gender, male)$ and $(John, speaks, Dutch)$. Experiments showed the best results when taking the relation-tail tuples that occur most often in the graph as classes. Alternatively, one could also use classes of the form $(head, relation, x)$, but on the datasets considered during evaluation, such head-relation tuples were rarer than relation-tail tuples, leading to worse predictions. This was mostly due to concept entities and other common entities appearing on the tail side. For example, although facts like $(Dutch, spoken~by, John)$ are theoretically possible, that fact would usually be stored as $(John, speaks, Dutch)$. Another Texter restriction concerns the number of classes. Representing every class that could be derived from the graph is practically impossible and would lead to unreasonable training times and bad performance due to unbalanced classes. Therefore, the number of classes is limited so that enough training data is available for the rarest classes.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{4_approach/1_texter/texter_idea}
    \caption{Basic idea behind the Texter: A multi-label classifier takes a query entity $x$'s texts and predicts relation-tail tuples that form facts when combined with the query entity as head.}
    \label{fig:4_approach/1_texter/texter_idea}
\end{figure}

A straight forward implementation of the described concept leads to the \emph{simple Texter}, which essentially consists of the embedding block presented in \autoref{subsec:4_approach/1_texter/1_text_embedding}, that embeds the input sentences, and the classification block introduced in \autoref{subsec:4_approach/1_texter/2_simple_model}, which produces the output logits. What the simple Texter is missing, is the desired prioritization between the entity's sentences. This feature is added by the more complex, \emph{attentive Texter} described in \autoref{subsec:4_approach/1_texter/3_attention_model}, which adds an attention mechanism between the embedding and classification blocks, that allows it to determine which sentence is most relevant for each class' prediction. That information is used in an attempt to improve the Texter's performance and returned as part of the prediction to provide the desired explanation to the user. Finally, \autoref{subsec:4_approach/1_texter/4_training} explains the training procedure. In \autoref{sec:5_experiments/3_texter}, both the simple and the attentive versions of the Texter are compared, whereby the simple Texter serves as a baseline to the attentive version.

\subsection{Text Encoding}
\label{subsec:4_approach/1_texter/1_text_embedding}
\input{4_approach/1_texter/1_text_embedding/text_embedding}

\subsection{Simple Model}
\label{subsec:4_approach/1_texter/2_simple_model}
\input{4_approach/1_texter/2_simple_model/simple_model}

\subsection{Attention Model}
\label{subsec:4_approach/1_texter/3_attention_model}
\input{4_approach/1_texter/3_attention_model/attention_model}

\subsection{Training}
\label{subsec:4_approach/1_texter/4_training}
\input{4_approach/1_texter/4_training/training}
