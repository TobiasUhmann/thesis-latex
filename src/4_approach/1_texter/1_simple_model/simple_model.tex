During inference, the first step of processing a query entity, which is the same for both the simple and the attentive Texter, is embedding the entity's sentences in the embedding block. Thereby, each sentence is processed individually as illustrated in Figure~\ref{fig:4_approach/1_texter/1_simple_model/simple_architecture}: First, the sentence is split into words by the tokenizer, which are handled as integer IDs in further processing. Next, the words are embedded using some NLP approach, which could be a simple lookup table in the simplest case. As the last step in the embedding block, the word embeddings are combined to sentence embeddings in the pooling layer. The resulting sentence embeddings, that capture the sentences' overall meanings, are then passed on to the classification block where each of them is pushed through the neural multi-label classifier which consists of a single linear layer. Another pooling layer then combines the sentence-wise classification logits to the entity's logits. Finally, applying the sigmoid function to the entity's logits yields the class probabilites that state the probabilities of the associated facts about the query entity.

\begin{figure}[t]
    \centering
    \includegraphics{4_approach/1_texter/1_simple_model/simple_architecture}
    \caption{Simple Texter Architecture; Each sentence is embedded and classified individually before the final pooling layer combines the results; "++++----" represents an embedding with positive and negative elements}
    \label{fig:4_approach/1_texter/1_simple_model/simple_architecture}
\end{figure}

Especially in the embedding block there are different possibilities for the concrete implementation of the individual parts to choose from, some of which will be examined in Chapter~\ref{ch:5_experiments}. In the final version of the power model, a transformer model is used for embedding the sentences' words, more precisely DistilBERT, a "distilled" variant of the transformer encoder BERT reduced to the essentials~\cite{Sanh2019DistilBERTAD}. In contrast to a simple lookup table, DistilBERT is able to incorporate the context of a word into its embedding, which leads to more meaningful sentence embeddings. This choice of embedding approach also affects the tokenizer, the pooling layer, and even the input sentences: Thanks to context embedding, transformers can use special tokens that add additional information to the text. While a classical model cannot decide on the basis of the sentence "Lisa likes John." whether this sentence speaks for the class $(x, has~gender, male)$, a transformer is able to do so given the appropriately marked sentence "Lisa likes [START]John[END].". Especially for long, ambigious sentences performance can be increased significantly.

Beyond the input data, the use of a transformer also affects the tokenizer and the pooling layer. The tokenizer has to apply byte pair encoding (BPE) that is commonly used by pre-trained transformers, where sentences are not only divided into words, but words are further divided into subwords, thus keeping the vocabulary of the tokenizer small and allowing to exploit homorphisms between similar words. In addition, the tokenizer inserts the [CLS] and [SEP] introduced by BERT at the beginning and end of the sentence. That way, "Lisa likes [START]John[END]." becomes "[CLS]Lisa likes [START]John[END].[SEP]". While the [SEP] token is used to separate sentences and has no further meaning here, the embedding of the [CLS] token captures the meaning of the whole sentence and is therefore  used as a sentence representation in the BERT paper~\cite{Devlin2019BERTPO}. However, the power experiments showed that performance can be increased if the word embeddings are also included, which is why the pooling layer of the embedding block averages all of a sentence's token embeddings, including the [CLS] embedding.

Less comprehensive processing steps happen in the classification block of the simple Texter: The sentence embeddings produced by the embedding block are pushed through the single linear classification layer whose multi-label output logits are averaged in the final pooling layer. The class probabilities calculated by applying the sigmoid function to the resulting entity logits are then used to sort the facts that have a probability greater than 50\%. Thus, the user of the model first receives the facts that are most likely to apply.
