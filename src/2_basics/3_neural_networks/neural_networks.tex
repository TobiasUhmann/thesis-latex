ml in general detects patterns in data
beside classic approaches like SVMs one import model type are neural nets
deep nets possible because much data nowadays
trained by forwarding input through net, calcing loss function and adjusting weights during backpropagation to minimize loss
during backpropagation sgd or similar
validate during training, keep out test data
hyperparameters, hyperparameter optimization
loss function: mse, cross entropy, binary cross entropy
feature/embedding space, manual feature engineering vs. automatic feature learning
eval/train mode (calc gradients or not)
softmax, sigmoid





NLP = natural language processing
machine learning, primer about training, datasets at <link> []

embedding based: embed tokens in (relatively) low-dim vector space,
that has properties like man/woman -> king/queen
word2vec, glove

pre-training
many works showed that pre-training on large general-purpose corpus with
subsequent finetuning yields better results~\cite{}

transformers
attention based
transformer = encoder + decoder
depending on problem one or both better suited, generally good: bert = encoder
bert revolutionalized nlp
