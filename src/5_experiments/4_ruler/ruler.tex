While training the Texter means repeatedly applying backpropagation to the Texter's neural network, the Ruler's equivalent is mining rules on the known train facts. The found rules are then used during inference to infer new facts from the ones known about a query entity. In contrast to the Texter's training, all training facts are used for rule mining and all test facts are considered during evaluation. Thus, the macro F1 and mAP metrics calculated over all test entities that were used for the contextual Texter's final evaluation are the default metrics applied to the Ruler.

For rule mining, 138k and 238k training facts are available in the CDE and FB splits, respectively. Depending on how long AnyBURL runs, more rules can be mined than the graph contains facts. The Ruler used in the final evaluation leveraged 1.74mio rules on the CDE split and 2.51mio rules on the FB split. Mining those rules took $t = 1000s$ on an Intel i7 quad-core processor. The vast majority of the rules have a rule body consisting of a single fact, such as the rule $(X, lives~in, Norway) => (X, speaks, English)$. Most of the rules have a confidence below the minimum confidence threshold $conf_{min} = 0.5$, meaning that the facts they imply are probably wrong. Those rules are therefore discarded, leaving 785k useful rules on CDE and 916k useful rules on the FB split. AnyBURL's default minimum support threshold $supp_{min} = 2$ was kept for the final evaluation.

The useful rules are then used for prediction by searching for rule groundings within the known facts, which include all train facts in addition to the query entity's known test facts. The heads of the rules for which a grounding could be found then yield the predicted facts. \autoref{tab:5_experiments/4_ruler/results} shows the final evaluation results obtained when evaluating the predictions against all test facts. The ``50'' suffix denotes that 50\% of the test facts may be used to apply rules during inference. As one would expect, the FB results are once again better than the CDE ones, due to the larger training set and the higher number of facts per entity.

\begin{table}[h]
    \centering
    \input{5_experiments/4_ruler/results}
    \caption{Final Ruler results on the CDE and FB splits with rules mined after $t = 1000s$ with $supp_{min} = 2$ and $conf_{min} = 0.5$}
    \label{tab:5_experiments/4_ruler/results}
\end{table}

In the following, \autoref{subsec:5_experiments/4_ruler/1_known} demonstrates how well the Ruler works in few shot scenarios, \autoref{subsec:5_experiments/4_ruler/2_rule_count} shows how much the number of mined rules affects the Ruler's performance, and \autoref{subsec:5_experiments/4_ruler/3_rule_quality} explains why removing low-quality rules leads to worse performance. All further evaluations were performed on Rulers that were trained on rule sets obtained from mining for $t = 100s$. \autoref{tab:5_experiments/4_ruler/2_rule_count/results} in \autoref{subsec:5_experiments/4_ruler/2_rule_count} can be used to estimate performances for longer mining times. The other training parameters are kept the same as for the final evaluation if not specified otherwise, i.e. $supp_{min} = 2$ and $conf_{min} = 0.5$.

\subsection{Known Test Facts}
\label{subsec:5_experiments/4_ruler/1_known}
\input{5_experiments/4_ruler/1_known/known}

\subsection{Number of Rules}
\label{subsec:5_experiments/4_ruler/2_rule_count}
\input{5_experiments/4_ruler/2_rule_count/rule_count}

\subsection{Rule Quality}
\label{subsec:5_experiments/4_ruler/3_rule_quality}
\input{5_experiments/4_ruler/3_rule_quality/rule_quality}
