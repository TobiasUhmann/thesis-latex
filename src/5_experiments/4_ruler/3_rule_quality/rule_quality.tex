Besides the number of mined rules, the rules' quality plays a crucial role concerning the Ruler's performance. By default, rules with support of at least $supp_{min} = 2$ and confidence of at least $conf_{min} = 0.5$ are considered. It was assumed that the Ruler's predictions might improve if the rules were further restricted to those with high support and confidence. Therefore, two independent experiments were conducted in which minimal support and confidence were gradually increased.

For the experiment on varying the minimal support threshold, it was expected that precision would not vary between low- and high-support rules, but that recall would decrease strongly due to a restriction to few high-support rules. Therefore, F1 and mAP were assumed to drop in accordance with recall. \autoref{tab:5_experiments/4_ruler/3_rule_quality/supp_results} shows the measurements for minimum support thresholds ranging from the default value of 2 up to 1000.

\begin{table}
    \makebox[\textwidth][c]{
        \input{5_experiments/4_ruler/3_rule_quality/supp_results}
    }
    \caption{Ruler results for various minimum support thresholds. Ruler uses rules mined after $t = 100s$ and keeps rules with $conf_{min} = 0.5$. Restricting to rules with high support does not improve performance.}
    \label{tab:5_experiments/4_ruler/3_rule_quality/supp_results}
\end{table}

The first numbers that stick out, but are not surprising, are the low numbers of high-confidence rules, especially on the CDE split. More interesting, however, is that precision does actually increase for high-support rules and recall does not suffer as much as expected, because few high-support rules seem to be able to handle many predictions on their own. But still, the decrease in recall outweighs the increase in precision which is why the other experiments, as well as the final evaluation, keep all useful rules regardless of their support.

Considering confidence, expectations were higher. It was assumed that the decreasing performance in the experiment on different rule mining times, presented in \autoref{subsec:5_experiments/4_ruler/2_rule_count}, was due to decreasing confidence of newly found rules and that this effect could be counteracted by increasing the minimum confidence threshold for rules. While this would reduce the number of usable rules, it was hoped that there were enough high-confidence rules to keep up the correct predictions. At a certain confidence threshold, a turning point was expected at which the decreasing recall would become too severe. Table A shows the results for confidence thresholds between 0.5 and 1.0.

\begin{table}
    \makebox[\textwidth][c]{
        \input{5_experiments/4_ruler/3_rule_quality/conf_results}
    }
    \caption{Ruler results for various minimum confidence thresholds. Ruler uses rules mined after $t = 100s$ and keeps rules with $supp_{min} = 2$. Restricting to rules with high confidence does not improve performance.}
    \label{tab:5_experiments/4_ruler/3_rule_quality/conf_results}
\end{table}

What stands out is the large number of rules with a confidence of 100\% which make up roughly a fifth of all rules on both fact splits. Overall, confidence values are distributed quite uniformly over rules with $conf >= 0.5$. As expected, precision increases significantly with confidence, but contrary to expectations, recall drops faster from the first confidence raise so that the initial confidence threshold of 0.5 is kept as the default.
