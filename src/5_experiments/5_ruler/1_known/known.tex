The assumption that 50\% of a test entity's facts are known during inference leads to stable evaluation results, especially if the data set provides only few facts per entity. However, if many facts are available, the assumption no longer corresponds to a few-shot scenario which is particularly interesting in practice. For a test entity from the FB split, 50\% known test facts already corresponds to an average of nine facts per entity.

To get an idea of how much the number of known test facts affects the Ruler's performance, this experiment evaluates all of the Power splits introduced in Section~\ref{sec:5_experiments/2_power_splits}, including the splits which include none or all of the entities' facts in the known test set. Test entities without known test facts correspond to open-world entities. Although, the Ruler cannot predict facts for them, they were processed to assure that the evaluation code would calculate precision and recall to 100\% and 0\%, respectively. The splits that include all of an entity's test facts as known facts does not represent a practical use case for the Ruler, neither. Still, it is included to show how much increasing the share of known facts beyond 50\% would bring. In practice, the most interesting splits are the ones with a low percentage of known test facts. The expectation was that recall would increase logarithmically with the number of known test facts, as the first fact about an entity reveals much more information than the last one, while precision should be independent from the choice of random known facts. Thus, F1 and mAP were expected to increase logarithmically with the proportion of known facts. Table~\ref{tab:5_experiments/5_ruler/1_unknown/all_results} shows the actual results.

\begin{table}
    \centering
    \input{5_experiments/5_ruler/1_known/results}
    \caption{Ruler test results for different Power splits on rules mined after $t = 100s$ with $supp_{min} = 2$ and $conf_{min} = 0.5$}
    \label{tab:5_experiments/5_ruler/1_unknown/all_results}
\end{table}

As expected, recall does increase with the number of test facts available for rule application. Precision, on the other hand does not stagnate, but decreases instead. The reason is the overlooked fact that true predictions about the same fact produced by rules based on multiple known test facts do not add up. In contrast, false predictions from multiple known facts usually do not overlap. However, the increase in recall prevails so that F1 increases with the number of known facts as does mean average precision. The results on the FB split are higher for low known facts shares because rounding off fact numbers affects the lower fact numbers on CDE more heavily than on the FB split. Recall does not equal zero on the "0" splits, because they contain entities without any test facts. In those cases the empty prediction set produced by the Ruler is correct and leads to 100\% recall.
