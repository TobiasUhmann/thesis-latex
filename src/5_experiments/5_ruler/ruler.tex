While training the Texter means repeatedly applying backpropagation to the Texter's neural network, the Ruler's equivalent is mining rules on the known train facts. The found rules are then used during inference to infer new facts from the few ones known about a query entity. In contrast to the Texter's training, all train facts are used for rule mining and all test facts are considered in the experiments - only one experiment deviates from this. Thus, the macro F1 and mAP metrics calculated over all test entities that were used for the contextual Texter's final results are the default metrics applied to the Ruler.

For rule mining, \num{137738} and \num{238191} training facts are available in the CDE and FB splits, respectively. Depending on how long AnyBURL runs, rule sets of a similar magnitude can be found. For the final evaluation, a runtime of 100s was set after which \num{235701} rules were found for the CDE and \num{299210} ones for the FB Split. The vast majority of these rules have a rule body consisting of a single fact, such as (lives in, Norway) => (speaks, English), for example.

The mined rules are then used for prediction by searching for rule groundings within the known facts, which include all train facts in addition to the query entity's known test facts. The heads of the rules for which a grounding could be found then yield the predicted facts. Table~\ref{tab:5_experiments/5_ruler/results} shows the evaluation results from preicting facts for all test entities in the CDE and FB splits and comparing them to the ground truth test facts. All of the Power splits introduced in Section~\ref{sec:5_experiments/2_power_datasets} are considered, including the splits which include none or all of the entities' test facts in the known test set. Test entities without known test facts correspond to open-world entities. Although, the Ruler cannot predict facts for them, they were processed to assure that the evaluation code would calculate precision and recall to 100\% and 0\%, respectively. The splits that include all of an entity's test facts as known facts does not represent a practical use case for the Ruler, neither. It has been included to show up the theoretical upper bound the Ruler can achieve. In practice, the most interesting use cases, in respect to the Ruler, include the splits with a low percentage of known test facts. It was expected that F1 and mAP would increase as the proportion of known test facts is increased. The results on the FB split should be better because  more train facts as well as more known test facts exist per entity.

% confidence distribution ?
% support distribution ?
% rule length distribution ?

\begin{table}
    \centering
    \input{5_experiments/5_ruler/results}
    \caption{Ruler evaluation against known+unknown. Codex 50 means 50\% known test facts}
    \label{tab:5_experiments/5_ruler/results}
\end{table}

rules found after 100s evaluated against valid facts
known valid facts used for predictions
could evaluate against unknown valid facts, close to use case, or could eval against all valid facts as usual []
ok to validate against known + unknown facts because known not seen during training
eval against unknown means filter out predicted known facts
table ? shows comparison between both eval methods
when evaluating against known + unknown, best result for 100
when evaluating against unknwon, best result for 50 as balance between known and remaining for prediction. 0 = open world = no preds, 100 = no left for prediction
results on FB better because more data

\subsection{Eval against unknown}
\label{subsec:5_experiments/5_ruler/1_unknown}
\input{5_experiments/5_ruler/1_unknown/unknown}

\subsection{More rules}
\label{subsec:5_experiments/5_ruler/2_rule_count}
\input{5_experiments/5_ruler/2_rule_count/rule_count}

\subsection{Rule Quality}
\label{subsec:5_experiments/5_ruler/3_rule_quality}
\input{5_experiments/5_ruler/3_rule_quality/rule_quality}
