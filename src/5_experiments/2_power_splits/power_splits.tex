Theoretically an IRT fact split and matching text set are sufficient for training the Power model. However, when evaluating the trained model against the IRT split's open-world entities, Power would perform only mediocre as it could only use its Texter component for inference, since the open-world entities do not provide any facts the Ruler can work with. Therefore, the IRT splits are transformed to \emph{Power splits}, which divide the validation and test facts into so-called \emph{known facts} and \emph{unknown facts}. During inference, only known facts may be used to apply rules, while both known and unknown facts form the ground truth during evaluation - neither known nor unknown validation and test facts are seen during rule mining. IRT separate closed-world training and validation facts are merged as there is no need for the latter. \autoref{fig:5_experiments/2_power_splits/power_split} illustrates the repartitioning.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{5_experiments/2_power_splits/power_split}
    \caption{Repartitioning an IRT fact split into a Power split by merging the training fact subsets and splitting open-world validation and test subsets into facts are known and unknown during inference, respectively.}
    \label{fig:5_experiments/2_power_splits/power_split}
\end{figure}

The percentage of known and unknown facts can be varied when creating the Power split to study the Ruler's effectiveness on entities with more or less given knowledge. \autoref{tab:5_experiments/2_power_splits/power_splits_table} lists the splits used for the evaluation. For readability, the Power splits based on the CoDEx-M and FB15k-237 splits are generally referred to and ``CDE'' and ``FB'', respectively. An integer suffix specifies the chosen percentage of known validation and test facts. For example, the CDE-50 Power split denotes the Power split created from the IRT CoDEx-M split with 50\% of the validation and test facts being available for rule application during inference. In practice, the CDE-15 and FB-5 splits are particullarly interesting, because they represent few-shot scenarios with only one or two known test facts per entity. The CDE-0 and FB-0 splits correspond to zero-shot open-world scenarios.

\begin{table}[t]
    \centering
    \input{5_experiments/2_power_splits/power_splits_table}
    \caption{Power splits with varying ratios of known validation and test fact. For example, ``CDE-50'' denotes the CoDEx-M-based Power split with half of the test facts being available for rule application during inference while the FB-0 Power split does not reveal any of the FB15k-237 facts during inference.}
    \label{tab:5_experiments/2_power_splits/power_splits_table}
\end{table}

Along with the creation of the Power split, the question of appropriate metrics for the evaluation arises. While evaluation scenarios for classical fact splits typically consider micro precision, recall and F1 across all facts, this work regards macro precision, recall, F1 and mAP averaged over the test entities, as this comes closer to the goal of producing top predictions for as many entities as possible -- while the micro metrics could achieve good results if the model focused on overall good predictions in favor of entities with few ground truth facts. It should be noted, however, that not further described experiments with the metrics yielded similar results to the macro ones. Mean average precison has been preferred over mean reciprocal rank as a ranking metric, because it does not focus on the single best prediction for an entity, which comes closer to the intended use case of suggesting multiple facts for KGC, as well. For entities for which no predictions are made, precision is mathematically undefined. In this case, the precision for that entity is set to 1 since no incorrect predictions were made. Similarly, recall and mAP are undefined when there are no ground truth facts for an entity. In these rare cases, recall and mAP are set to 1 if no predictions were made for the entity and 0 otherwise.
