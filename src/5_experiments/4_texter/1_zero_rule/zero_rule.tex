Zero rule baselines are naive models that give an idea of how well a problem could be tackled by a naive approach, such as simply guessing random solutions. For example, in a coin toss, one would already achieve 50\% accuracy, precision and recall by guessing between heads and tails. In the case of unbalanced classes, even higher accuracy can be achieved by permanently setting to the most probable class. The empirical data in Table~\ref{tab:5_experiments/4_texter/1_zero_rule/results} are in the magnitude of the theoretically calculated values. For example, for a binary classification problem with possible outcomes $y = 0$ and $y = 1$, accuracy would be calculated as $P(\hat{y} = y) = P(\hat{y} = 0) \cdot P(y = 0) + P(\hat{y} = 1) \cdot P(y = 1)$ where $P(\hat{y} = 0)$ and $P(\hat{y} = 1)$ depend on the guessing strategy. The values in~\ref{tab:5_experiments/4_texter/1_zero_rule/results} were gathered by running each of the four zero rule strategies ten times and averaging the values. For each zero rule strategy, performance is shown for the most common class and the least common class in addition to the average over all classes. In regard to the training data it should be noted that the most common and least common classes occur more often than in the test data sets. On the CDE test set the most and least common classes have frequencies of 39.86\% and 2.38\%, respectively, while the gap on the FB split is milder with 34.52\% and 1.97\%, respectively.

\begin{table}[h]
    \makebox[\textwidth][c]{
        \input{5_experiments/4_texter/1_zero_rule/results}
    }
    \caption{Zero rule baselines}
    \label{tab:5_experiments/4_texter/1_zero_rule/results}
\end{table}

The workings of the four zero rule strategies can be summarized as follows: Uniformly sampling predictions from the set of possible results implies a 50/50 chance for predicting true or false in a binary classification scenario. It leads to 50\% accuracy and recall, independent from the class frequency. In combination with precision values proportional to the class frequencies, the resulting F1 score is the highest on the CDE and FB splits, compared to the other zero rule strategies. Stratified sampling shifts the sampling of predictions in favor of the more frequent values in the training data. For a class with true/false ratio of 80/20, the chance of predicting true/false changes from 50/50 to 80/20. This results in higher accuracy, due to more true negatives, but lower recall as less true positives are hit. Since precision is not affected, F1 decreases with the recall. If the goal was to achieve both, as many true positives as well as true negatives, the strategy of simply predicting the most frequent value in the test data, in this case 0, would be most promising as it would lead to an accuracy near 100\% for spare datasets. For KGC, however, this implies a recall of 0 for any entity that true positives exist for and thus an average F1 score of nearly 0. Vice versa, the recall becomes 100\% when constantly predicting true for every class, but precision will drop, leading to a lower F1 score compared to uniform sampling.

Due to the open-world assumption, i.e. the knowledge graph does not include information about false facts, the Power model cannot predict that facts are false. Therefore, precision, recall and F1 score, which focus on the prediction of true positives are decisive for the evaluation. Accuracy is merely provided for comprehensiveness and to justify the usage of other zero rule baselines in Table~\ref{tab:5_experiments/4_texter/1_zero_rule/results}. Therefore, the lower measuring bars for the Texter are 20.19\% F1 score, set by the uniformly sampling zero rule baseline, for the CDE split and 9.43\% F1 score, set by the constantly true predicting zero rule baseline, for the FB split.
