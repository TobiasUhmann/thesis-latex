First, the embedding block in both, the simple and the attentive Texter, is investigated when using static, pre-trained word embeddings, i.e. a word is embedded independent from where it occurs in the sentence. The advantages include ease of implementation using a simple look-up table, few hyperparameters and quick training. However, the sentence embedding cannot properly capture the meaning of markings and maskings via special tokens such as those in sentences from the "cde-irt-5-marked" or "cde-irt-5-masked" text sets exemplified in Table~\ref{tab:5_experiments/1_data_sources/2_text_sets/text_sets_table}. Therefore, all experiments in this section are conducted on the clean text sets containing no special tokens. During training, the settings that proved to be best during the experiments were used. In particular, these are the use of an NLP-based tokenizer, pre-trained fastText~\ref{} word embeddings that are finetuned during training, mean pooling to form sentence embeddings, the sigmoid function in the attention block in the case of the attentive texter, class weights in the computation of the loss, and the Adam optimizer to apply gradients during backpropagation. Table~\ref{tab:5_experiments/4_texter/2_static/results} shows the final results over all clean text sets for both the simple and the attentive versions of the Texter. As for all following tables presenting Texter evaluation results, for compactness, only the name of the text set is given for the respective Texter dataset, because it implies the matching Power split.

\begin{table}[t]
    \centering
    \input{5_experiments/4_texter/2_static/results}
    \caption{Final evluation results for the simple and the attentive Texter using static word embeddings - for each text set, the better model is marked in terms of precision, recall and F1}
    \label{tab:5_experiments/4_texter/2_static/results}
\end{table}

The numbers show that both the simple and the attentive Texter, outperform the zero rule baselines by far. However, the attentive model does not outperform the simple version as hoped. On most datasets, the attentive model achieves better precision, but the simple Texter has higher recall. Overall, the simple modell performs better, especially on datasets with multiple sentences per entity which should have been in favor of the attentive model. Interestingly, both Texters perform best given the short OWE sentence due to high recall scores, which implies that a single, good entity description is more useful than many vague contexts for knowledge graph completion. Similar to the OWE sentences, the CDE sentences from the entities' Wikipedia introductions perform best on the CDE. However, the results are not far ahead of those for the text set with 30 IRT sentences, so that the observation could also be formulated in reverse: 15 randomly sampled entity contexts are just as good as a selected high-quality description.

In addition to the final evaluation results on the test data, Figure~\ref{fig:5_experiments/4_texter/2_static/plot_valid_curves} shows the development of the F1 score for different text sets during training on the FB graph. After 200 episodes of training, the simple and attentive versions of the Texter converge against similar values. The only notable difference between the two is that the simple model reaches its optimal training time earlier before it starts overfitting on most datasets. Thereby, the IRT text sets with few sentences are more prone to overfitting than the ones with 15 or 30 sentences per entity. The short OWE texts, on the other hand, seem to be immune to overfitting. Although training could be stopped after 50 episodes without major defficiencies in this case, all following experiments are performed with 200 episodes to ensure that all models reach their full potential even if changes lead to slower training.

\begin{figure}[t]
    \centering
    \input{5_experiments/4_texter/2_static/plot_valid_curves/plot_valid_curves}
    \caption{Validation F1 curves during training the Texter on the FB split with 1 OWE sentence (red), 30 IRT sentences (orange), 15 IRT sentences (yellow), 5 IRT sentences (black) or 1 IRT sentence (green) per entity}
    \label{fig:5_experiments/4_texter/2_static/plot_valid_curves}
\end{figure}

What is not visible in Figure~\ref{fig:5_experiments/4_texter/2_static/plot_valid_curves} is the information how the average F1 score is composed of the class-wise F1 scores, which is addressed by Figure~\ref{fig:5_experiments/4_texter/2_static/plot_class_curves}. It compares the F1 scores between the most common classes, the least common classes, and the average F1 score over all classes. The graphs reveal that predictions for common classes are much more reliable. The three most common classes on the CDE split, with frequencies of almonst 20\% each, reach performances between 50\% and 80\%, whereas the three least common classes, with frequencies of around 1\% each, reach strongly divergent values, from 0\% to 60\%. Despite the tendency that frequent classes perform better, however, a class' performance cannot be calculated directly from its frequence. The least common classes have very similar frequencies but perform very differently. For the second least common class prediction does not work at all, while the third least common class performs even better than the third most common class. There are also significant performance differences between the three most frequent classes although the frequencies are similar. Another, more obvious, finding is that frequent classes are learned within fewer episodes than less common classes. Interestingly, the attentive Texter seems to learn faster than the simple Texter. Finally, the graph shows that both Texters converge to similar values for all classes, as might have been assumed from the similar macro F1 values.

\begin{figure}[t]
    \centering
    \input{5_experiments/4_texter/2_static/plot_class_curves/plot_class_curves}
    \caption{Class-wise validation F1 curves during training the Texter on the CDE split with 5 IRT sentences per entity - comparison between the most common classes (red, orange, yellow), the least common classes (green, blue, purple) and the average value over all classes (black)}
    \label{fig:5_experiments/4_texter/2_static/plot_class_curves}
\end{figure}

Looking at the results for the other datasets shown in Tables~\ref{fig:a_appendix/static_classes_1}~--~\ref{fig:a_appendix/static_classes_3} in Appendix~\ref{ch:a_appendix}, similar patterns can be observed on other text sets with only a few exceptions. However, although there are no major differences between the simple and the attentive Texter, it is noticeable that the results for rare classes do not improve steadily when increasing the number of sentences in case of the IRT text sets. Multiple training runs on the same data showed that this was not due to different initializations of the model. Instead, separate experiments showed that it is the random selection of IRT sentences that affects the performance of individual rare classes. In the respective experiment, new fb-irt-1-clean text sets were built by repeatedly selecting random sentences from the larger fb-irt-30-clean dataset. The overall F1 score over all classes, however, stayed the same for all runs.

With the final results for static word embeddings at hand, the following subsections show what impact various model changes and hyperparameters have. The affected model components are examined in the order in which they are invoked during training, beginning with the tokenizer and ending with the optimizer. Finally, the attentive Texter's attention mechanism is investigated to explain why it does not improve upon the simple Texter as hoped.

\subsubsection{Changing the tokenizer}
\label{subsubsec:5_experiments/4_texter/2_static/1_tokenizer}
\input{5_experiments/4_texter/2_static/1_tokenizer/tokenizer}

\subsubsection{Initializing word embeddings randomly}
\label{subsubsec:5_experiments/4_texter/2_static/2_emb_size}
\input{5_experiments/4_texter/2_static/2_emb_size/emb_size}

\subsubsection{Using pre-trained word embeddings}
\label{subsubsec:5_experiments/4_texter/2_static/3_pre_trained}
\input{5_experiments/4_texter/2_static/3_pre_trained/pre_trained}

\subsubsection{Freezing pre-trained embeddings}
\label{subsubsec:5_experiments/4_texter/2_static/4_update_vectors}
\input{5_experiments/4_texter/2_static/4_update_vectors/update_vectors}

\subsubsection{Pooling}
\label{subsubsec:5_experiments/4_texter/2_static/5_pooling}
\input{5_experiments/4_texter/2_static/5_pooling/pooling}

\subsubsection{Activation Function}
\label{subsubsec:5_experiments/4_texter/2_static/6_activation}
\input{5_experiments/4_texter/2_static/6_activation/activation}

\subsubsection{Appplying class weights}
\label{subsubsec:5_experiments/4_texter/2_static/7_weight_factor}
\input{5_experiments/4_texter/2_static/7_weight_factor/weight_factor}

\subsubsection{Choosing an optimizer}
\label{subsubsec:5_experiments/4_texter/2_static/8_optimizer}
\input{5_experiments/4_texter/2_static/8_optimizer/optimizer}

\subsubsection{Inspecting the attention mechanism}
\label{subsubsec:5_experiments/4_texter/2_static/9_attention}
\input{5_experiments/4_texter/2_static/9_attention/attention}
