The switch to DistilBert also opens up new possibilities for the embedding block's pooling layer. Although, after the experiments on static word embeddings, the choice of mean pooling remains, there are now several possibilities which embeddings to average. As mentioned before, DistilBERT adds the [CLS] token to the input sentence. The purpose of this token is to capture the meaning of the sentence, which is what it is used for in the BERT paper. According to reports from various online forums, however, models achieved better results by using the means of the actual word embedding instead of the [CLS] embedding. Therefore, it was tested which variant works better for the Power model. Table~\ref{tab:5_experiments/4_texter/3_context/2_pooling/grid_searc} shows the results, referring to the word embeddings as "Words". While on it, two further variants were tested, namely averaging all of the mentioned embeddings, that is the word embeddings plus the [CLS] embedding, and averaging all embeddings including those for the padding tokens. Those two variants are referred to as "All" and "Pad" in Table~\ref{tab:5_experiments/4_texter/3_context/2_pooling/grid_searc}.

\begin{table}[h]
    \centering
    \input{5_experiments/4_texter/3_context/2_pooling/grid_search}
    \caption{Evaluation results for contextual Texters using various approaches towards the pooling layer - all entries show the macro F1 over all classes, the best results per text set are in bold}
    \label{tab:5_experiments/4_texter/3_context/2_pooling/grid_search}
\end{table}

Apparently, however, the difference between the approaches is negligable on all text sets. The involvement of all word embeddings plus the [CLS] embedding seems to yield minimally better results than the other approaches, so it has been set as the default pooling strategy.
