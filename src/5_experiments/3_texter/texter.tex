With a Power split in place and the evaluatin metrics defined, the next step is training the Power model's components. In case of the Texter, however, an intermediate step is necessary. The Texter is trained with respect to a certain set of relation-tail tuples that make up its classes and requires a fixed number of sentences per entity. Therefore, an intermediate \emph{Texter dataset} is created from a Power split and a matching text set consisting of one row per entity, holding the entity's sentences and specifying which of the selected classes hold for that entity. \autoref{tab:5_experiments/3_texter/texter_dataset} shows the an example Texter dataset with eight classes and one sentence per entity. Experiments proved most successful when taking the most common relation-tail tuples from the considered Power split. For the experiments and the evaluation on the CDE and FB splits, the 100 most common relation-tail tuples were selected, because they cover over half of all facts in both splits and the rarest classes still occur often enough to provide enough training data. As for the number of sentences, the maximum number of sentences per entity available in the respective text set was taken. For example, when creating a Texter dataset from the CDE split and the cde-irt-5-clean text set, all of the entities' 5 sentences were shuffled and taken over into the Texter dataset. The few entities that had less sentences in the text set were left out of the Texter dataset.

\begin{table}
    \centering
    \input{5_experiments/3_texter/texter_dataset}
    \caption{Excerpt from a Texter dataset. For each entity, a fixed number of sentences is given and a sparse, binary array specifies which Texter classes hold.}
    \label{tab:5_experiments/3_texter/texter_dataset}
\end{table}

As \autoref{tab:5_experiments/3_texter/texter_dataset} indicates, the Texter dataset's classes array is usually very sparse, especially for diverse knowledge graphs without frequently occurring relation-tail tuples. Therefore, it is important to weight the classes when calculating the loss during training to punish false negatives more heavily than false positives. Otherwise, the Texter would learn that always predicting 0 keeps the loss low. \autoref{tab:5_experiments/3_texter/classes} gives an overview of the imbalance between the top 100 classes that were selected for evaluation. As can be seen, the FB split contains not only more facts than the CDE split, but also fewer diverse facts.

\begin{table}[h]
    \centering
    \input{5_experiments/3_texter/classes}
    \caption{Most and least common classes on the CDE and FB splits. The denser FB15k-237 graph leads to more frequent classes in the FB split.}
    \label{tab:5_experiments/3_texter/classes}
\end{table}

Like the Power split it was built from, the Texter dataset is split into training, validation and test subsets in accordance with the Power split's training, validation and test entities. In contrast to the Power split, the Texter dataset does not contain all of the entities' facts which is why a different evaluation metric is used. Instead of measuring entity-wise precision, recall and F1 and averaging over all entities, the Texters precision, recall and F1 are calculated class-wise and averaged over all classes, which it harder for Texter, since good metrics on common classes are dragged down by equally-weighted rare classes, but fits the goal of predicting all classes as good as possible. For the final comparison with Ruler and Aggregator, however, the Texter is also evaluated against the Power split's test facts, including the facts the Texter is unable to predict.

The following \autoref{subsec:5_experiments/3_texter/1_zero_rule} applies the presented metrics to several baselines to give an idea of the problem's complexity. Given the reference values from the baselines, \autoref{subsec:5_experiments/3_texter/2_static} and~\ref{subsec:5_experiments/3_texter/3_context} present two ways of implementing the Texter's embedding block using static and contextual word embeddings, whereby the latter is used in the final Power implementation described in \autoref{ch:4_approach}. To distinguish the two variants, they are also referred to as static and contextual Texter, respectively. In addition, each experiment compares the simple and the attentive versions of the Texter to see which variations affect the attention mechanism of the latter.

\subsection{Zero Rule Baselines}
\label{subsec:5_experiments/3_texter/1_zero_rule}
\input{5_experiments/3_texter/1_zero_rule/zero_rule}

\subsection{Static Word Embeddings}
\label{subsec:5_experiments/3_texter/2_static}
\input{5_experiments/3_texter/2_static/static}

\subsection{Contextual Word Embeddings}
\label{subsec:5_experiments/3_texter/3_context}
\input{5_experiments/3_texter/3_context/context}
