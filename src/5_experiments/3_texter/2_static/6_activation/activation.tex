In case of the attentive Texter, the sentence embeddings generated by the embedding block are subsequently passed on to the attention block where they are scalar multiplied with the class embeddings to obtain the attention values that specify how well a class embedding matches each of the entity's sentences. For normalization purposes a classes' attentions are further pushed through a non-linear activation function before the resulting values serve as weight factors in calculating the class-specific entity embeddings returned from the attention block.

Initially, the softmax function was considered as the activation function, so that the attention mechanism is forced to compare all of an entities sentencies to each other. On the other hand, it is problematic that the attention weights sum to 1 even if none of the sentences, or all of them, fit the class. In the first case, the class embedding converges towards unrelated sentence embeddings during backpropagation and in the second case several insightful sentences are not fully utilized, because each one's gradient is lower than possible. Therefore, the sigmoid function was considered as an alternative, allowing each sentence embedding to be weighted by its attention value in $[0, 1]$ individually. While on it, the gererally popular ReLu function was also tested. Out of interest in the principle utility of a non-linear activation function, it was also measured what happens when no activation function is applied at all. Both the relu function and the omission of an activation function allow outliers with a large scalar product to have great influence on the learning process, which was expected to have a negative impact. In the comparison between Softmax and Sigmoid, the outcome was uncertain. \autoref{tab:5_experiments/3_texter/2_static/6_activation/grid_search} shows the results of varying the activation function for the attentive Texter only, as the simple model does not have an attention block.

\begin{table}[t]
    \centering
    \input{5_experiments/3_texter/2_static/6_activation/grid_search}
    \caption{Static Texters with various activation functions in the attention block. Numbers show F1 scores. Best result per row marked bold. The sigmoid function works best. Using no activation function works surprisingly well.}
    \label{tab:5_experiments/3_texter/2_static/6_activation/grid_search}
\end{table}

In line with expectations, the Sigmoid and Softmax functions produce the best results. For all text sets that provide multiple sentences per entity, however, Sigmoid is ahead by a few percentage points. On single-sentence text sets Sigmoid and Softmax perform similiarly. Interestingly, not using an activation function performs only slightly worse than Softmax on the CDE split and even similarly well on the FB split. On the text set, even the best results are achieved without an activation function. The otherwise promising ReLu function was inferior to the other functions in this experiment. For all further experiments, the sigmoid function was used as the standard.
