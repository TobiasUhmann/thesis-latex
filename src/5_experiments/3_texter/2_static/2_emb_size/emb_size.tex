Upon tokenization, the sentences' tokens are embedded -- in the context of this chapter using static word embeddings. For this, randomly initialized, or as shown in the next subsection, pre-trained embeddings can be used. When using randomly initialized embeddings, the questions of what embedding size and which random distribution to chose arises. As random distribution the standard normal distribution $N(0, 1)$ is set. The optimal embedding size is determined via grid search. It is related to, among other things, the amount of available training data and is determined via grid search. Small embeddings can hold little specific information and generalize well, whereas large embeddings can represent subtleties of individual tokens but are also more susceptible to overfitting. The grid search covers a wide range of embedding sizes around the expected optimal embedding size of about 200, a usual size in current models, including unrealistically small sizes to find out to what degree it is worth increasing the size. Overall, expectations for the experiment were limited because of the moderately large amount of training. When using the maximum embedding size of 1000, performance was expected to decrease due to overfitting. \autoref{tab:5_experiments/3_texter/2_static/2_emb_size/grid_search} shows the experiment's results.

\begin{table}[t]
    \centering
    \input{5_experiments/3_texter/2_static/2_emb_size/grid_search}
    \caption{Static Texter with randomly initialized word embeddings of varying size. Numbers show F1 scores. Best result per row marked bold. The simple Texter profits from very large embeddings, while the attentive Texter's performance decreases from medium sizes on.}
    \label{tab:5_experiments/3_texter/2_static/2_emb_size/grid_search}
\end{table}

At first glance, one can see that training does actually work and that the simple Texter performs much better with randomly initialized embeddings. Against expectations, the attentive Texter reaches its top performance at an embedding size of only around 30 to 100, while the simple model benefits from very large embeddings beyond 300. Apart from one outlier, the performances of both models evolve similarly for small embedding sizes until the attentive Texter stagnates early at medium sizes. For the attentive model, the poor performance seems reasonable since semantically similar words, onto which the same class embedding should match, have completely different initial values, which should complicate the class embeddings' training. Another aspect that catches the eye when looking at \autoref{tab:5_experiments/3_texter/2_static/2_emb_size/grid_search} is that even one-dimensional embeddings deliver surprisingly good results. In the case of the CDE split, the appearance is deceptive, as the F1 values are actually below the best zero-rule baseline, which reaches about 20\%, but for the FB split, the results are indeed better than the best zero-rule baseline with only 9\%. Overall, it can be said that randomly initialized embeddings can work, but, despite the good results for the simple Texter, they are not considered in further experiments, as pre-trained embeddings yield better results for both models as shown in the next subsection.
