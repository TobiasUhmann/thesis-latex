\begin{table}
    \centering
    \input{5_experiments/3_texter/2_static/4_update_vectors/grid_search}
    \caption{Evaluation results for static Texters when (not) freezing the pre-trained embeddings - all entries show the macro F1 over all classes, the best results per text set are in bold}
    \label{tab:5_experiments/3_texter/2_static/4_update_vectors/grid_search}
\end{table}

When using pre-trained word embeddings, the embeddings adapt to the new training data during fine-tuning and may lose their special properties~\cite{He2019AnalyzingTF}. To prevent this, the new training data fine-tuning is sometimes mixed in with the training data used during pre-training. Another option is to freeze the pre-trained word embeddings during training, so that other parameters must align themselves more strongly while the word embeddings remain constant. The latter approach was considered in an attempt to enhance the attentive Texter's class embeddings. The outcome of the experiment was uncertain. On the one hand, the model is deprived of a large part of its parameters, leaving only the class embeddings and linear layers to be learned, on the other hand, it could focus the training on the class embeddings and prevent overfitting. Table~\ref{tab:5_experiments/3_texter/2_static/4_update_vectors/grid_search} shows the results. In addition to the fasttext.simple.300d embeddings, charngram.300d and glove.6B.300d were also examined, as it was conceivable that embedding freezing would have different effects depending on the embedding type.

As the numbers inevitably show, freezing the pre-trained embeddings results in worse performance for every dataset and every type of embedding which is why no further experiments make use of it. Nevertheless, it is interesting to observe how different the negative effects are: CharNGram shows immense performance losses while GloVe embeddings handle the restriction well on text sets with few IRT sentences per entity. Nonetheless, even if the negative effects of cannot be compensated, the assumption that freezing the word embeddings has a positive effect on the attention mechanism seems to be valid, since the attentive Texter's performance drops less on text sets with multiple IRT sentences compared to the simple model.
