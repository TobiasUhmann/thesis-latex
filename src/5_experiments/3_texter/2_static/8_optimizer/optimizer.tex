\begin{table}[t]
    \centering
    \input{5_experiments/3_texter/2_static/8_optimizer/grid_search}
    \caption{Static Texter when applying different learning rates during training. Numbers show F1 scores. Best result per row marked bold. The simple Texter can be trained with high learning rates while the attentive Texter is more sensible, especially on datasets with many sentences per entity.}
    \label{tab:5_experiments/3_texter/2_static/8_optimizer/grid_search}
\end{table}

To apply the gradients calculated with respect to the loss function, an optimizer is used. As described in \autoref{ch:4_approach}, the adaptive AdamW~\cite{Loshchilov2019DecoupledWD} optimizer has finally been used for this purpose, due to its recommendation for training transformers. For training the static Texter, however, two different optimizers, namely standard Adam~\cite{Kingma2015AdamAM} and SGD with momentum have been considered~\cite{Qian1999OnTM}. Thereby, the adaptive Adam optimizer has been chosen because it is generally considered a fast and good optimizer~\cite{AdamPopular}, while SGD with momentum has been tried because, as a non-adaptive optimizer, it is said to find a better solution for the loss function~\cite{Wilson2017TheMV} -- although it might take longer. In the experiment, SGD's momentum constant was set to 0.9.

Directly related to the optimizer is the learning rate. Both optimizers were tested with different learning rates from a range that should allow training within a reasonable time. It was expected that lower learning rates always lead to a better result given sufficient training time, but that the improvements become smaller and smaller as the learning rate decreases so that a sufficiently good learning rate can be declared for each optimizer. SGD with momentum was expected to perform slightly better after a longer training time. Depending on the ratio of additional training time to gained performance, a decision was to be made between the optimizers.

\begin{figure}[t]
    \centering
    \input{5_experiments/3_texter/2_static/8_optimizer/sgd_vs_adam/sgd_vs_adam}
    \caption{Static Texter optimized via SGD and Adam with high to low learning rates (red = 1.0, orange, yellow, black, green, blue, purple = 0.001) on the fb-owe-1-clean text set. The Adam optimizer converges much faster but does not work with high learning rates when training the attentive Texter.}
    \label{fig:5_experiments/3_texter/2_static/8_optimizer/sgd_vs_adam/sgd_vs_adam}
\end{figure}

Unfortunately, even at the highest learning rate, SGD converged too slowly to estimate definitive results. Therefore, \autoref{tab:5_experiments/3_texter/2_static/8_optimizer/grid_search} only shows the results for Adam. The four plots in \autoref{fig:5_experiments/3_texter/2_static/8_optimizer/sgd_vs_adam/sgd_vs_adam} illustrate the training of the simple and the attentive Texter using SGD and Adam with different learning rates. The upper plots show SGD's slowly converging curves while Adam's plots below paint a contrary picture. Even with the highest learning rate, SGD is slower than Adam with the lowest learning rate and would most likely need at least 200 more episodes to converge. An optimizer-independent observation is that the complex model appears to be more sensitive to learning rates that are too high. For SGD the highest learning rate leads to slower training progress and for Adam the training does not work at all at the two highest learning rates. This information is also reflected in \autoref{tab:5_experiments/3_texter/2_static/8_optimizer/grid_search}, which also reveals another fact that is not directly evident from the plot: A lower learning rate does not automatically result in better performance. However, the nearest explanation is that training is simply not finished at that point. Furthermore, \autoref{tab:5_experiments/3_texter/2_static/8_optimizer/grid_search} suggests that the training of the attentive Texter is somewhat slower, but at a learning rate of 0.003, the training of both models appears to converge after the specified 200 epochs, which is why this learning rate is used in the other experiments.
