\begin{table}[t]
    \makebox[\textwidth][c]{
        \input{5_experiments/3_texter/2_static/1_tokenizer/static_tokenizer_alt}
    }
    \caption{Static Texter with either whitespace or SpaCy tokenizer. Numbers show F1 scores. Best result per row marked bold. Using SpaCy always yields better results, especially for the attentive Texter.}
    \label{tab:5_experiments/3_texter/2_static/1_tokenizer/static_tokenizer_alt}
\end{table}

The first variable component in both the simple and the attentive Texter is the tokenizer. A naive tokenizer might split the input sentences on any whitespace such as spaces, tabs, and line breaks. The problem with that simple approach is the large number of resulting tokens blowing up the vocabulary. For example, the sentence ``Hello, world!'' would be split into the two tokens ``Hello,'' and ``world!'', which would be different from ``Hello'' and ``world''. As a consequence, none of a word's ``impure'' occurrences contribute to learning the words embedding, but rather offer the model a way to overfit. One possible solution to the problem would be deleting punctuation characters, numbers, dates, and other ``polluting'' characters and unique tokens. However, this could also lead to the destruction of correct tokens, such as ``U.K.''. Instead, this work uses a tokenizer from the NLP library SpaCy~\cite{SpaCy} which splits the sentence ``Welcome to the U.K.!'' into the tokens ``Welcome'', ``to'', ``the'', ``U.K.'', and ``!''.

It was expected that an upgrade from tokenizing on whitespace to using an NLP library would particularly benefit the Texter when being trained on the CDE and IRT sentences sampled from Wikipedia since they are not tailored to serve a machine learning model. Especially, the first sentences from a Wikipedia page often contain parentheses or phonetic symbols that could mess up adjacent words if they are not separated from them. However, the IRT text sets with many sentences were expected to profit less from a better tokenizer, since the loss of some training tokens should be negligible compared to the increased amount of training data. \autoref{tab:5_experiments/3_texter/2_static/1_tokenizer/static_tokenizer_alt} shows the measured evaluation results.

As assumed, an advanced tokenizer does increase the Texter's performance on every dataset, whereby the improvement is only marginal on the fb-owe-1-clean text set with its short entity descriptions. The text sets with IRT sentences benefit most from the SpaCy tokenizer -- even the text sets with 30 sentences per entity. Furthermore, it seems that the attentive Texter benefits slightly more from SpaCy than the simple model.
