The first variable component in both the simple and the attentive Texter is the tokenizer. A naive tokenizer might split the input sentences on any whitespace such as spaces, tabs and line breaks. The problem with that simple approach is the large number of resulting tokens blowing up the vocabulary. For example, the sentence "Hello, world!" would be split into the two tokens "Hello," and "world!", which would be different from "Hello" and "world". As a consequence, none of a word's "impure" occurrences contribute to learning the words embedding, but rather offer the model a way to overfit. One possible solution would be deleting punctuation characters, numbers, dates and other "polluting" characters and unique tokens. However, this could also lead to the destruction of correct tokens, such as "U.K.". Instead, this work uses a tokenizer from the NLP library SpaCy \cite{SpaCy} which splits the sentence "Welcome to the U.K.!" into the tokens "Welcome", "to", "the", "U.K." and "!".

%\begin{table}[h]
%    \centering
%    \input{tables/experiments/static/grid_search.tex}
%    \caption{Impact of different tokenizers on vocabulary size and evaluation results}
%    \label{table:experiments/static/tokenizer}
%\end{table}

Table~\ref{table:experiments/static/tokenizer} shows the tokenizer choice's impact on the vocabulary size and the evaluation results for two text sets on FB15k-237. For the large text set "fb-irt-30", the vocabulary's size halves when using the SpaCy tokenizer rather than splitting on whitespace and the F1 score increases slightly. On "fb-owe-1", the vocabulary's size increases by only 30\%, but the performance gap is over 5\%, probably because every "lost" token means a bigger loss on the small text set. Table~\ref{table:static_tokenizers_all} in Appendix~\ref{ch:a_appendix} contains the full evaluation over all text sets where similar results can be observed on the "cde-irt-30" and "cde-cde-1" text sets.
