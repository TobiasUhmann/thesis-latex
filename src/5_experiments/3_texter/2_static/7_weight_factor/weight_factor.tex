After the entity embeddings have been formed from the sentence embeddings in the attention block, they are passed through the classification block which outputs the final class logits of the forward() function. In case of the simple model, the attention block is omitted and the sentence embeddings are used directly for classification. In either case, the forward() function's class logits are taken as input by the loss function during training to calculate the model's loss in regard to the ground truth class labels.

% TODO moved to approach

\begin{align}
    wBCE(x, y) = - \frac{1}{C} \sum_{c = 1}^C w_c \cdot log(\sigma(x)) + (1 - y) \cdot log(1 - \sigma(x))
    \label{eq:5_experiments/3_texter/2_static/7_weight_factor/wbce}
\end{align}

In an experiment, whose results fill Table~\ref{tab:5_experiments/3_texter/2_static/7_weight_factor/grid_search}, it should be verified that the use of class weights has a positive effect. Furthermore, it should be ensured that the choice to calculate the weights as the reciprocal of the class frequencies is optimal. Therefore, also smaller and larger weights were included in the comparison by halving and doubling the classes' reciprocals, respectively.

\begin{table}[t]
    \centering
    \input{5_experiments/3_texter/2_static/7_weight_factor/grid_search}
    \caption{Static Texters when multiplying the class weights with different factors. Numbers show F1 scores. Best result per row marked bold. The "None" column shows the results when applying no class weights at all. Applying class weights in the first place is more important than the weight factor.}
    \label{tab:5_experiments/3_texter/2_static/7_weight_factor/grid_search}
\end{table}

The empirical values are in line with the theoretical expectations. Although omitting class weights produces useful values - especially on the FB split where the 100 most frequent classes have higher frequencies than on the CDE split - applying class weights significantly improve performance. Thereby, it seems more important to apply weights at all than to fine-tune the weights. In most cases, it does not matter much whether the weights are halved or doubled. On average, the plain reciprocals seem to be the best choice, which is why they are set as the default.
