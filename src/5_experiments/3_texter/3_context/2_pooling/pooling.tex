The switch to DistilBert also opens up new possibilities for the embedding block's pooling layer. Although, after the experiments on static word embeddings, the choice of mean pooling remains, with a transformer, there are several possibilities which embeddings to average. As mentioned before, DistilBERT adds the [CLS] token to the input sentence, whose purpose is to capture the meaning of the sentence. Thus, the easiest way to receive a sentence embedding in the pooling layer is to simply take the [CLS] token's embedding, as it is also recommended in the BERT paper~\cite{Devlin2019BERTPO}. According to reports from various online forums, however, models achieved better results by using the mean of the actual word embedding instead. Therefore, it was tested which variant works better for the Power model. \autoref{tab:5_experiments/3_texter/3_context/2_pooling/grid_search} shows the results. While on it, two further variants were tested, namely averaging both the [CLS] embedding and the word embeddings, and averaging all embeddings including those created for the padding tokens.

\begin{table}[t!]
    \centering
    \input{5_experiments/3_texter/3_context/2_pooling/grid_search}
    \caption{Contextual Texters using various pooling methods. Sentence embeddings can be created by taking the [CLS] token's embedding (CLS), averaging the words' embeddings (Words), averaging the words' embeddings, including the [CLS] token (C+W), or by averaging all token embeddings, including embedded paddings (C+W+P). Overall, the choice does not make a big difference.}
    \label{tab:5_experiments/3_texter/3_context/2_pooling/grid_search}
\end{table}

Apparently, however, the difference between the approaches is negligible on all text sets. The involvement of all word embeddings plus the [CLS] embedding seems to yield minimally better results than the other approaches, so it has been set as the default pooling strategy.
