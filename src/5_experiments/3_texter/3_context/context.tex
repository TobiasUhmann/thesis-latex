Modern NLP models continue where classical models with static word embedding reach their limits when it comes to long sentences in which the relationship between the words is even more important for the words', and thus, the overall sentence's meaning. For this purpose, the particularly successful transformers use internal attention mechanisms that learn for each word which other words in the sentence are particularly relevant and should therefore be included in the word's embedding. This results in a context-dependent embedding for each occurrence of the word.

Besides embedding usual words and some standard tokens that represent unknown words and paddings, the here used DistilBERT also supports the special [CLS] and [SEP] tokens introduced by the BERT model. The [CLS] token's purpose is to capture the meaning of the sentence as a whole during training. In addition, the Texter supports the definition of custom tokens, such as the ones used for markings or maskings in the IRT text sets. This additional information should lead to a significant performance increase, even when the entity mention is masked, as it preserves the information what the surrounding sentence is about.

For the Texter's downstream architecture, the change from a lookup table for embeddings to the use of DistilBERT does not mean a big change as it only affects the embedding block -- the classification block and, in the case of the attentive version, the attention block remain untouched. In particular, design choices from the experiments with static word embeddings, such as the usage of the sigmoid function in the attention block, are kept. During training, however, the optimizer has to be adjusted to accommodate the deep transformers within the embedding block.

\begin{table}
    \makebox[\textwidth][c]{
        \input{5_experiments/3_texter/3_context/results}
    }
    \caption{Final evaluation of the contextual Texter on all text sets. Results of the static Texter are given for comparison. The contextual Texter is evaluated against the Texter dataset's test subset (F1) and against all facts from the respective split (F1 all, mAP all). The contextual Texter outperforms the simple Texter in general, especially when leveraging markings in the text. The attentive Texter profits more from contextual word embeddings. Still, the simple Texter performs better in terms of mAP.}
    \label{tab:5_experiments/3_texter/3_context/results}
\end{table}

After the switch to DistilBERT, training of the Texter takes longer due to the large number of parameters in the transformer, but can be terminated after 50 epochs. \autoref{tab:5_experiments/3_texter/3_context/results} shows the evaluation results for the final Texter after that time. In addition to the previously regarded macro F1 score over all classes, \autoref{tab:5_experiments/3_texter/3_context/results} also provides F1 and mAP over the entities from the Power split -- including those the Texter cannot predict. In addition to the contextual Texters' evaluation results, the static Texters' final results from \autoref{subsec:5_experiments/3_texter/2_static} are given for comparison where available. Furthermore, \autoref{ch:a_appendix} contains the detailed \autoref{tab:a_appendix/context_final_prec_rec} that provides the precision and recall values.

The final evaluation results reveal some interesting facts: First, the contextual Texter performs better on clean text sets than the static Texter most of the time, but not always. While the contextual Texter performs better on almost all text sets containing IRT sentences, it does not for the CDE and OWE sentences. Second, the CDE and OWE text sets show best, that the attentive Texter benefits more from contextual word embeddings than the simple model. Third, it is obvious that markings bring great improvements, as was expected, while the masked text sets are in between the clean and marked ones in terms of F1. In terms of mAP, however, the masked text sets lead to better predictions than their marked counterparts in multiple cases. Fourth, the evaluating against all facts from the Power split leads to significantly worse results on the FB split than it does on the CDE split, which is probably due to the reason that the FB classes cover a smaller portion of the larger FB split.

Overall, it can be stated, that the Texter performs better when using contextual word embeddings, especially when marked texts are available. Therefore, the contextual Texter is the default in the Power model. However, depending on the given text set as well as hardware support, static word embeddings might be a noteworthy alternative. When comparing the simple and attentive versions of the contextual Texter, the simple version yields slightly better results. Still, the attentive Texter is kept due to its ability to explain its text-based decision to a certain point.

In the following, subsections~\ref{subsubsec:5_experiments/3_texter/3_context/1_sent_len}~--~\ref{subsubsec:5_experiments/3_texter/3_context/3_optimizer} will look at some experiments on the updated embedding block and the adjusted optimizer used to train the deep contextual Texter.

\subsubsection{Varying sentence length}
\label{subsubsec:5_experiments/3_texter/3_context/1_sent_len}
\input{5_experiments/3_texter/3_context/1_sent_len/sent_len}

\subsubsection{Pooling}
\label{subsubsec:5_experiments/3_texter/3_context/2_pooling}
\input{5_experiments/3_texter/3_context/2_pooling/pooling}

\subsubsection{Optimizer}
\label{subsubsec:5_experiments/3_texter/3_context/3_optimizer}
\input{5_experiments/3_texter/3_context/3_optimizer/optimizer}
