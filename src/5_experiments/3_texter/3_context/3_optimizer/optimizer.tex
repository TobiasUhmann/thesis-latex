For deep models, and thus also for transformers, and thus the contextual Texter, it is particularly important to choose a good optimizer. While the adaptive Adam optimizer was a good substitute for SGD for the static Texter, the Adam-based AdamW optimizer is recommended for the fine-tuning of transformers~\cite{Loshchilov2019DecoupledWD}. According to its inventors, it keeps the speed of Adam, while approaching the generalization capabilities of SGD with momentum. To find the correct value for the sensitive learning rate of the AdamW optimizer, values in the range of the implementation's default value of $10^{-3}$ and a value of $10^{-7}$ recommended were tried. Table~\ref{tab:5_experiments/3_texter/3_context/2_pooling/grid_search} shows the results for the learning rates that led to useful results.

\begin{table}[h]
    \centering
    \input{5_experiments/3_texter/3_context/3_optimizer/grid_search}
    \caption{Evaluation results for contextual Texters trained with various learning rates - all entries show the macro F1 over all classes, the best results per text set are in bold}
    \label{tab:5_experiments/3_texter/3_context/3_optimizer/grid_search}
\end{table}

By deviating from the default learning rate, great improvements could be achieved by lowering the learning rate to $3 \cdot 10^{-5}$, especially on the text sets with many IRT sentences where training did not work at all before. Furthe decreasing the learning rate led to slightly worse performance again - probably, because the model would have taken much more time to converge.
