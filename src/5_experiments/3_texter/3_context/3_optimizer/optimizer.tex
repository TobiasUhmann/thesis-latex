For deep models, such as transformers, and thus the contextual Texter, it is particularly important to choose a good optimizer. While the adaptive Adam optimizer was a good substitute for SGD for the static Texter, the Adam-based AdamW optimizer is recommended for the fine-tuning of transformers~\cite{Loshchilov2019DecoupledWD}. According to its inventors, it keeps the speed of Adam, while approaching the generalization capabilities of SGD with momentum. To find the correct value for the sensitive learning rate, values in the range of the implementation's default value of $10^{-3}$ and a value of $10^{-7}$ have been tried. \autoref{tab:5_experiments/3_texter/3_context/2_pooling/grid_search} shows the results for the learning rates that led to useful results within a reasonable training time.

\begin{table}[h]
    \centering
    \input{5_experiments/3_texter/3_context/3_optimizer/grid_search}
    \caption{Contextual Texters trained with various learning rates. Numbers show F1 scores. Best value per row marked bold. Setting the learning rate too high can have a devastating effect on performance.}
    \label{tab:5_experiments/3_texter/3_context/3_optimizer/grid_search}
\end{table}

By deviating from the default learning rate, great improvements could be achieved. Lowering the learning rate to $3 \cdot 10^{-5}$ not only improved performance but made training possible at all on the text sets with 30 IRT sentences per entity. Further decreasing the learning rate led to slightly worse performance -- probably, because the model required more time to converge.
