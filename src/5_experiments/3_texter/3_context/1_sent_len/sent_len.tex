While the sentence length, or more precisely the number of tokens, is not a critical variable for the static texter, since it is not  expensive to simply pad all sentences to a sufficient length, this would lead to a significantly longer training time for the contextual Texter. Unfortunately, some of the randomly selected IRT sentences are significantly longer than average sentences and would lose much of their information when being shortened. Therefore, this experiment measures the sentence length from which on this loss is negligible in order to keep the input token sequences for DistilBERT as short as possible. Table A shows the results for some sentence lengths. Sentence lengths beyond 64 BPE-Tokens did not lead to improvements greater 1\%.

\begin{table}[h]
    \centering
    \input{5_experiments/3_texter/3_context/1_sent_len/grid_search}
    \caption{Contextual Texters when capping the input sentences after various numbers of BPE tokens. Numbers show F1 scores. Best value per row marked bold. Generally, considering more tokens improves performance, but capping at 32 tokens already yields good results, especially on text sets with short sentences.}
    \label{tab:5_experiments/3_texter/3_context/1_sent_len/grid_search}
\end{table}

Surprisingly, even sentences cut to a length of only 16 tokens led to nearly optimal results on text sets with five sentences or more - only the cde-cde-1-clean and cde-irt-1-marked text sets cannot handle such short sentences. For the particularly short OWE sentences, the attentive Texter even records better results than if they were inflated by padding. For the final model, 64 for chosen as the default sentence length.
