% TODO 0 gt and 0 pred -> 100 prec, 0 rec, 0 f1. explains prec in certain (which?) cases

Before training and evaluating Power's Texter and Ruler on the Power Datasets over the following sections, this section explains the choice of common evaluation metrics that all components are measured by. In addition, the Texter is evaluated in a way that makes the test results comparable to the validation metrics during training throughout most experiments in Section~\ref{sec:5_experiments/4_texter} and Section~\ref{sec:5_experiments/5_ruler} contains one experiment that uses a slightly adjusted evaluation, as well. Nonetheless, the final versions of Texter, Ruler and Aggregator are all evaluated equally against the same test set, after all.

The metrics of choice are macro precision, recall and F1 score, abbreviated as \emph{PRF} in the following, over all test entities and mean average precision (mAP) of the predicted facts. Considered alternatives included micro PRF, as well as mean reciprocal rank (MRR). The reason for those choices is the target use case in which a user performs manual knowledge graph completion with the help of Power's recommendations. In that scenario it is desirable to provide multiple high-ranked recommendation as many entities as possible. Contrary to macro PRF, micro PRF would not punish comprehensive predictions on few entities while performing poorly on others. MRR was abandoned in favor of mAP, since the latter rewards an overall good ranking of all top facts where MRR only aims at ranking the top true positive as high as possible. It should be noted, however, that not further described experiments with micro PRF yielded similar results as macro PRF.
